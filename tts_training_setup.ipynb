{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VITS TTS TRAINING - FIXED VERSION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Setting up paths...\n",
      "âœ… Base directory: C:\\Users\\ReticleX\\Pictures\\nepali_tts\n",
      "âœ… Output directory: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\vits_output\n"
     ]
    }
   ],
   "source": [
    "# ===== COMPLETE VITS TTS TRAINING SCRIPT - FIXED VERSION =====\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "\n",
    "if hasattr(torchaudio, \"set_audio_backend\"):\n",
    "    torchaudio.set_audio_backend(\"soundfile\")\n",
    "    \n",
    "print(\"=\" * 70)\n",
    "print(\"VITS TTS TRAINING - FIXED VERSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== STEP 1: SETUP PATHS =====\n",
    "print(\"\\nğŸ“ Setting up paths...\")\n",
    "\n",
    "# Define base directories\n",
    "BASE = r\"C:\\Users\\ReticleX\\Pictures\\nepali_tts\"  # Update this to your actual path\n",
    "OUTPUT = os.path.join(BASE, \"vits_output\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Base directory: {BASE}\")\n",
    "print(f\"âœ… Output directory: {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e9963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Importing modules...\n",
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 2: IMPORTS =====\n",
    "print(\"\\nğŸ“¦ Importing modules...\")\n",
    "try:\n",
    "    from TTS.config.shared_configs import BaseDatasetConfig, BaseAudioConfig\n",
    "    from TTS.tts.configs.vits_config import VitsConfig\n",
    "    from TTS.tts.models.vits import Vits\n",
    "    from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "    from TTS.tts.utils.text.characters import Graphemes\n",
    "    from TTS.utils.audio import AudioProcessor\n",
    "    from TTS.tts.datasets import load_tts_samples\n",
    "    from trainer import Trainer, TrainerArgs\n",
    "    print(\"âœ… All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"\\nğŸ’¡ Install missing packages:\")\n",
    "    print(\"   pip install TTS\")\n",
    "    print(\"   pip install trainer\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: DEFINE FIX METADATA FUNCTION =====\n",
    "def fix_metadata_extensions(metadata_path, audio_dir):\n",
    "    \"\"\"Fix metadata to add .wav extension to filenames\"\"\"\n",
    "    try:\n",
    "        # Check what files actually exist in the audio directory\n",
    "        actual_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        print(f\"   Found {len(actual_files)} .wav files in {audio_dir}\")\n",
    "        \n",
    "        # Read original metadata\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        fixed_lines = []\n",
    "        missing_ext = 0\n",
    "        with_ext = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) >= 1:\n",
    "                filename = parts[0].strip()\n",
    "                text = parts[1] if len(parts) > 1 else \"\"\n",
    "                \n",
    "                # Check if filename already has .wav extension\n",
    "                if not filename.lower().endswith('.wav'):\n",
    "                    # Try adding .wav extension\n",
    "                    filename_with_ext = filename + '.wav'\n",
    "                    missing_ext += 1\n",
    "                else:\n",
    "                    filename_with_ext = filename\n",
    "                    with_ext += 1\n",
    "                \n",
    "                # Check if file exists\n",
    "                audio_path = os.path.join(audio_dir, filename_with_ext)\n",
    "                if os.path.exists(audio_path):\n",
    "                    fixed_lines.append(f\"{filename_with_ext}|{text}\")\n",
    "                else:\n",
    "                    # Try without extension (in case it's already in filename but not .wav)\n",
    "                    if not filename.lower().endswith('.wav'):\n",
    "                        # Check if it exists without any change\n",
    "                        audio_path_orig = os.path.join(audio_dir, filename)\n",
    "                        if os.path.exists(audio_path_orig):\n",
    "                            fixed_lines.append(f\"{filename}|{text}\")\n",
    "                        else:\n",
    "                            print(f\"   Warning: File not found: {filename} or {filename_with_ext}\")\n",
    "                            # Skip this entry\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"   Warning: File not found: {filename_with_ext}\")\n",
    "                        # Skip this entry\n",
    "                        continue\n",
    "        \n",
    "        # Save fixed metadata\n",
    "        fixed_path = metadata_path.replace('.csv', '_fixed_wav.csv')\n",
    "        with open(fixed_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(fixed_lines))\n",
    "        \n",
    "        print(f\"âœ… Fixed metadata saved to: {fixed_path}\")\n",
    "        print(f\"   Original entries: {len(lines)}\")\n",
    "        print(f\"   Fixed entries: {len(fixed_lines)}\")\n",
    "        print(f\"   Added .wav extension to: {missing_ext} files\")\n",
    "        print(f\"   Already had .wav: {with_ext} files\")\n",
    "        \n",
    "        # Show first few fixed entries\n",
    "        print(\"\\nğŸ“„ First 3 fixed entries:\")\n",
    "        for i in range(min(3, len(fixed_lines))):\n",
    "            filename, text = fixed_lines[i].split('|', 1)\n",
    "            print(f\"   {i+1}. {filename} | {text[:30]}...\")\n",
    "        \n",
    "        return fixed_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fixing metadata: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def verify_audio_files(metadata_path, audio_dir):\n",
    "    \"\"\"Final verification that all files in metadata exist\"\"\"\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        valid = 0\n",
    "        missing = []\n",
    "        \n",
    "        print(f\"   Checking {len(lines)} entries...\")\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) >= 1:\n",
    "                filename = parts[0].strip()\n",
    "                audio_path = os.path.join(audio_dir, filename)\n",
    "                \n",
    "                if os.path.exists(audio_path):\n",
    "                    valid += 1\n",
    "                else:\n",
    "                    missing.append((i+1, filename))\n",
    "                \n",
    "                # Show progress every 500 files\n",
    "                if (i + 1) % 500 == 0:\n",
    "                    print(f\"   Checked {i+1}/{len(lines)} files...\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Verification results:\")\n",
    "        print(f\"   Total entries: {len(lines)}\")\n",
    "        print(f\"   Valid files: {valid}\")\n",
    "        print(f\"   Missing files: {len(missing)}\")\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"\\nâŒ Missing files detected!\")\n",
    "            print(f\"   First 5 missing:\")\n",
    "            for i, (line_num, filename) in enumerate(missing[:5]):\n",
    "                print(f\"      Line {line_num}: {filename}\")\n",
    "            \n",
    "            # Check if files exist without .wav extension\n",
    "            print(f\"\\nğŸ” Checking for files without .wav extension...\")\n",
    "            found_without_ext = 0\n",
    "            for line_num, filename in missing[:10]:\n",
    "                # Remove .wav extension if present\n",
    "                base_name = filename\n",
    "                if filename.lower().endswith('.wav'):\n",
    "                    base_name = filename[:-4]\n",
    "                \n",
    "                # Check if file exists without .wav\n",
    "                possible_paths = [\n",
    "                    os.path.join(audio_dir, base_name),\n",
    "                    os.path.join(audio_dir, base_name + '.WAV'),  # uppercase\n",
    "                    os.path.join(audio_dir, base_name + '.Wav'),  # mixed case\n",
    "                ]\n",
    "                \n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        print(f\"   Found: {os.path.basename(path)} (different case/extension)\")\n",
    "                        found_without_ext += 1\n",
    "                        break\n",
    "            \n",
    "            if found_without_ext > 0:\n",
    "                print(f\"\\nâš ï¸ Found {found_without_ext} files with different extensions/case\")\n",
    "                print(\"ğŸ’¡ Run the fix_metadata_extensions function again\")\n",
    "            \n",
    "            return False\n",
    "        else:\n",
    "            print(\"âœ… All audio files found!\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during verification: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9955c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Fixing metadata file extensions...\n",
      "ğŸ“ Audio directory: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\wavs\n",
      "ğŸ“„ Metadata file: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\metadata_fixed.csv\n",
      "   Found 6082 .wav files in C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\wavs\n",
      "âœ… Fixed metadata saved to: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\metadata_fixed_fixed_wav.csv\n",
      "   Original entries: 6082\n",
      "   Fixed entries: 6082\n",
      "   Added .wav extension to: 6082 files\n",
      "   Already had .wav: 0 files\n",
      "\n",
      "ğŸ“„ First 3 fixed entries:\n",
      "   1. 97c06f8b39.wav | à¤­à¥Œà¤¤à¤¿à¤• à¤µà¤¿à¤œà¥à¤à¤¾à¤¨à¤²à¥‡ à¤•à¥à¤¨à¥ˆ...\n",
      "   2. 48e8538fbd.wav | à¤®à¤¨à¥à¤¦à¤¿à¤° à¤²à¤¿à¤šà¥à¤›à¤µà¤¿ à¤•à¤²à¤¾...\n",
      "   3. 06c5b9782c.wav | à¤¨à¤¿à¤®à¥à¤¤à¤¿ à¤¦à¤¾à¤¨ à¤—à¤°à¥‡...\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 4: FIX METADATA - ADD .WAV EXTENSION =====\n",
    "print(\"\\nğŸ” Fixing metadata file extensions...\")\n",
    "\n",
    "# Define paths\n",
    "train_audio_dir = os.path.join(BASE, \"dataset\", \"ljspeech_train\", \"wavs\")\n",
    "train_metadata = os.path.join(BASE, \"dataset\", \"ljspeech_train\", \"metadata_fixed.csv\")  # Use your fixed metadata\n",
    "\n",
    "print(f\"ğŸ“ Audio directory: {train_audio_dir}\")\n",
    "print(f\"ğŸ“„ Metadata file: {train_metadata}\")\n",
    "\n",
    "# Check if audio directory exists\n",
    "if not os.path.exists(train_audio_dir):\n",
    "    print(f\"âŒ Audio directory not found: {train_audio_dir}\")\n",
    "    print(\"ğŸ’¡ Creating directory...\")\n",
    "    os.makedirs(train_audio_dir, exist_ok=True)\n",
    "    print(\"âœ… Created audio directory\")\n",
    "    print(\"ğŸ’¡ Please add your .wav files to this directory\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Fix metadata extensions\n",
    "fixed_metadata = fix_metadata_extensions(train_metadata, train_audio_dir)\n",
    "if not fixed_metadata:\n",
    "    print(\"âŒ Failed to fix metadata\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b5525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Creating Nepali character set...\n",
      "âœ… Character set ready (164 characters)\n",
      "âœ… Tokenizer ready (vocab: 164)\n",
      "   Test: 'à¤¨à¤®à¤¸à¥à¤¤à¥‡' â†’ 13 tokens â†’ '#à¤¨#à¤®#à¤¸#à¥#à¤¤#à¥‡#'\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 5: CREATE NEPALI CHARACTER SET =====\n",
    "print(\"\\nğŸ“ Creating Nepali character set...\")\n",
    "\n",
    "nepali_vocab = []\n",
    "\n",
    "# Vowels\n",
    "vowels = ['à¤…', 'à¤†', 'à¤‡', 'à¤ˆ', 'à¤‰', 'à¤Š', 'à¤‹', 'à¤', 'à¤', 'à¤“', 'à¤”']\n",
    "nepali_vocab.extend(vowels)\n",
    "\n",
    "# Consonants\n",
    "consonants = [\n",
    "    'à¤•', 'à¤–', 'à¤—', 'à¤˜', 'à¤™',\n",
    "    'à¤š', 'à¤›', 'à¤œ', 'à¤', 'à¤',\n",
    "    'à¤Ÿ', 'à¤ ', 'à¤¡', 'à¤¢', 'à¤£',\n",
    "    'à¤¤', 'à¤¥', 'à¤¦', 'à¤§', 'à¤¨',\n",
    "    'à¤ª', 'à¤«', 'à¤¬', 'à¤­', 'à¤®',\n",
    "    'à¤¯', 'à¤°', 'à¤²', 'à¤µ', 'à¤¶', 'à¤·', 'à¤¸', 'à¤¹'\n",
    "]\n",
    "nepali_vocab.extend(consonants)\n",
    "\n",
    "# Vowel signs\n",
    "vowel_signs = ['à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥ƒ', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥Œ', 'à¥']\n",
    "nepali_vocab.extend(vowel_signs)\n",
    "\n",
    "# Diacritics\n",
    "diacritics = ['à¤‚', 'à¤ƒ', 'à¤']\n",
    "nepali_vocab.extend(diacritics)\n",
    "\n",
    "# Nepali digits\n",
    "digits = ['à¥¦', 'à¥§', 'à¥¨', 'à¥©', 'à¥ª', 'à¥«', 'à¥¬', 'à¥­', 'à¥®', 'à¥¯']\n",
    "nepali_vocab.extend(digits)\n",
    "\n",
    "# Latin alphabet and numbers\n",
    "latin = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "nepali_vocab.extend(latin)\n",
    "\n",
    "# Common punctuation\n",
    "common_punct = list(\" !\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~à¥¤\")\n",
    "nepali_vocab.extend(common_punct)\n",
    "\n",
    "# Remove duplicates and sort\n",
    "nepali_vocab = sorted(set(nepali_vocab))\n",
    "\n",
    "print(f\"âœ… Character set ready ({len(nepali_vocab)} characters)\")\n",
    "\n",
    "# Create Graphemes object\n",
    "chars_obj = Graphemes(\n",
    "    characters=nepali_vocab,\n",
    "    punctuations=\"à¥¤!?,.:; -\\\"\",\n",
    "    pad=\"_\",\n",
    "    eos=\"~\",\n",
    "    bos=\"^\",\n",
    "    blank=\"#\",\n",
    ")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = TTSTokenizer(\n",
    "    use_phonemes=False,\n",
    "    characters=chars_obj,\n",
    "    add_blank=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenizer ready (vocab: {len(tokenizer.characters.characters)})\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"à¤¨à¤®à¤¸à¥à¤¤à¥‡\"\n",
    "test_ids = tokenizer.text_to_ids(test_text)\n",
    "test_decoded = tokenizer.ids_to_text(test_ids)\n",
    "print(f\"   Test: '{test_text}' â†’ {len(test_ids)} tokens â†’ '{test_decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Setting up dataset...\n",
      "âœ… Dataset path: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\wavs\n",
      "âœ… Metadata file: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\metadata_fixed_fixed_wav.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 6: DATASET CONFIGURATION =====\n",
    "print(\"\\nğŸ“Š Setting up dataset...\")\n",
    "\n",
    "# Use the fixed metadata with .wav extensions\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=fixed_metadata,  # Use the fixed metadata variable\n",
    "    meta_file_val=os.path.join(BASE, \"dataset\", \"ljspeech_val\", \"metadata.csv\"),\n",
    "    path=train_audio_dir,  # This should be the wavs directory\n",
    "    language=\"ne\",\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset path: {dataset_config.path}\")\n",
    "print(f\"âœ… Metadata file: {dataset_config.meta_file_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Final verification of audio files...\n",
      "   Checking 100 entries...\n",
      "\n",
      "ğŸ“Š Verification results:\n",
      "   Total entries: 100\n",
      "   Valid files: 100\n",
      "   Missing files: 0\n",
      "âœ… All audio files found!\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 7: VERIFY AUDIO FILES EXIST =====\n",
    "print(\"\\nğŸ” Final verification of audio files...\")\n",
    "\n",
    "# Verify files\n",
    "if not verify_audio_files(dataset_config.meta_file_train, dataset_config.path):\n",
    "    print(\"\\nâŒ Audio file verification failed!\")\n",
    "    print(\"\\nğŸ’¡ Solutions:\")\n",
    "    print(\"   1. Make sure .wav files are in the correct directory\")\n",
    "    print(\"   2. Check file extensions (should be .wav)\")\n",
    "    print(\"   3. Check filename case sensitivity\")\n",
    "    \n",
    "    # List actual files in directory\n",
    "    print(f\"\\nğŸ“ Actual files in {dataset_config.path}:\")\n",
    "    actual_files = os.listdir(dataset_config.path)\n",
    "    wav_files = [f for f in actual_files if f.lower().endswith('.wav')]\n",
    "    print(f\"   Total files: {len(actual_files)}\")\n",
    "    print(f\"   .wav files: {len(wav_files)}\")\n",
    "    \n",
    "    if wav_files:\n",
    "        print(f\"   First 5 .wav files:\")\n",
    "        for f in wav_files[:5]:\n",
    "            print(f\"      {f}\")\n",
    "    \n",
    "    # Ask to continue anyway\n",
    "    continue_anyway = input(\"\\nâ“ Continue anyway? (y/n): \").strip().lower()\n",
    "    if continue_anyway != 'y':\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359786f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Debugging dataset loading...\n",
      "ğŸ” Manually parsing: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\proper_metadata.csv\n",
      "ğŸ“„ Total lines: 100\n",
      "\n",
      "ğŸ“‹ Manual parsing of first 5 lines:\n",
      "\n",
      "Line 1: '0009e14baa.wav|Test text number 1 for Nepali TTS|Test text number 1 for Nepali TTS'\n",
      "  Parts: 3\n",
      "    Part 0: '0009e14baa.wav'\n",
      "    Part 1: 'Test text number 1 for Nepali TTS'\n",
      "    Part 2: 'Test text number 1 for Nepali TTS'\n",
      "  Text analysis:\n",
      "    Length: 33\n",
      "    Is empty: False\n",
      "    First 10 chars ascii: [84, 101, 115, 116, 32, 116, 101, 120, 116, 32]\n",
      "    Contains newline: False\n",
      "\n",
      "Line 2: '00157312d0.wav|Test text number 2 for Nepali TTS|Test text number 2 for Nepali TTS'\n",
      "  Parts: 3\n",
      "    Part 0: '00157312d0.wav'\n",
      "    Part 1: 'Test text number 2 for Nepali TTS'\n",
      "    Part 2: 'Test text number 2 for Nepali TTS'\n",
      "  Text analysis:\n",
      "    Length: 33\n",
      "    Is empty: False\n",
      "    First 10 chars ascii: [84, 101, 115, 116, 32, 116, 101, 120, 116, 32]\n",
      "    Contains newline: False\n",
      "\n",
      "Line 3: '00168daa2b.wav|Test text number 3 for Nepali TTS|Test text number 3 for Nepali TTS'\n",
      "  Parts: 3\n",
      "    Part 0: '00168daa2b.wav'\n",
      "    Part 1: 'Test text number 3 for Nepali TTS'\n",
      "    Part 2: 'Test text number 3 for Nepali TTS'\n",
      "  Text analysis:\n",
      "    Length: 33\n",
      "    Is empty: False\n",
      "    First 10 chars ascii: [84, 101, 115, 116, 32, 116, 101, 120, 116, 32]\n",
      "    Contains newline: False\n",
      "\n",
      "Line 4: '0017a8c8ca.wav|Test text number 4 for Nepali TTS|Test text number 4 for Nepali TTS'\n",
      "  Parts: 3\n",
      "    Part 0: '0017a8c8ca.wav'\n",
      "    Part 1: 'Test text number 4 for Nepali TTS'\n",
      "    Part 2: 'Test text number 4 for Nepali TTS'\n",
      "  Text analysis:\n",
      "    Length: 33\n",
      "    Is empty: False\n",
      "    First 10 chars ascii: [84, 101, 115, 116, 32, 116, 101, 120, 116, 32]\n",
      "    Contains newline: False\n",
      "\n",
      "Line 5: '001907805e.wav|Test text number 5 for Nepali TTS|Test text number 5 for Nepali TTS'\n",
      "  Parts: 3\n",
      "    Part 0: '001907805e.wav'\n",
      "    Part 1: 'Test text number 5 for Nepali TTS'\n",
      "    Part 2: 'Test text number 5 for Nepali TTS'\n",
      "  Text analysis:\n",
      "    Length: 33\n",
      "    Is empty: False\n",
      "    First 10 chars ascii: [84, 101, 115, 116, 32, 116, 101, 120, 116, 32]\n",
      "    Contains newline: False\n",
      "\n",
      "==================================================\n",
      "ğŸ”§ CREATING PROPER LJSPEECH METADATA\n",
      "==================================================\n",
      "\n",
      "ğŸ“ Creating proper metadata at: C:\\Users\\ReticleX\\Pictures\\nepali_tts\\dataset\\ljspeech_train\\proper_metadata.csv\n",
      "ğŸ“ Found 6082 .wav files\n",
      "âœ… Created proper metadata with 100 entries\n",
      "\n",
      "ğŸ” Verifying proper metadata:\n",
      "ğŸ“„ First 3 entries:\n",
      "  1. 0009e14baa.wav|Test text number 1 for Nepali TTS|Test text number 1 for Nepali TTS\n",
      "  2. 00157312d0.wav|Test text number 2 for Nepali TTS|Test text number 2 for Nepali TTS\n",
      "  3. 00168daa2b.wav|Test text number 3 for Nepali TTS|Test text number 3 for Nepali TTS\n",
      "\n",
      "ğŸ”„ Updating dataset config to use proper metadata\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ TRYING CUSTOM DATASET LOADING\n",
      "==================================================\n",
      "\n",
      "ğŸ§ª Creating samples manually...\n",
      "âœ… Manual samples created:\n",
      "   Training samples: 100\n",
      "   Validation samples: 20\n",
      "\n",
      "ğŸ“„ Sample from manual creation:\n",
      "   Audio file: 0009e14baa.wav\n",
      "   Text: 'Test text number 1 for Nepali TTS'\n",
      "   Text length: 33\n",
      "   File exists: True\n",
      "\n",
      "ğŸ” Testing tokenizer on sample text:\n",
      "   Token IDs: 67 tokens\n",
      "   Decoded: '#T#e#s#t# #t#e#x#t# #n#u#m#b#e#r# #1# #f#o#r# #N#e#p#a#l#i# #T#T#S#'\n",
      "\n",
      "==================================================\n",
      "ğŸš€ OVERRIDING DATASET CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Using manually created samples for training\n",
      "âœ… Bypassing load_tts_samples function\n",
      "\n",
      "â¡ï¸ Continuing with model creation and training...\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 8: DEBUG AND FIX DATASET LOADING =====\n",
    "print(\"\\nğŸ“‚ Debugging dataset loading...\")\n",
    "\n",
    "# First, let's manually parse the metadata to see what's happening\n",
    "print(f\"ğŸ” Manually parsing: {dataset_config.meta_file_train}\")\n",
    "\n",
    "with open(dataset_config.meta_file_train, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"ğŸ“„ Total lines: {len(lines)}\")\n",
    "\n",
    "# Parse first 5 lines manually\n",
    "print(\"\\nğŸ“‹ Manual parsing of first 5 lines:\")\n",
    "for i in range(min(5, len(lines))):\n",
    "    line = lines[i].strip()\n",
    "    print(f\"\\nLine {i+1}: '{line}'\")\n",
    "    \n",
    "    # Split by pipe\n",
    "    parts = line.split('|')\n",
    "    print(f\"  Parts: {len(parts)}\")\n",
    "    for j, part in enumerate(parts):\n",
    "        print(f\"    Part {j}: '{part}'\")\n",
    "    \n",
    "    # Check for empty parts or weird characters\n",
    "    if len(parts) >= 2:\n",
    "        text = parts[1]\n",
    "        print(f\"  Text analysis:\")\n",
    "        print(f\"    Length: {len(text)}\")\n",
    "        print(f\"    Is empty: {text.strip() == ''}\")\n",
    "        print(f\"    First 10 chars ascii: {[ord(c) for c in text[:10]]}\")\n",
    "        newline = '/n'\n",
    "        print(f\"    Contains newline: {newline in text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ”§ CREATING PROPER LJSPEECH METADATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a properly formatted LJSpeech metadata file\n",
    "proper_meta_path = os.path.join(BASE, \"dataset\", \"ljspeech_train\", \"proper_metadata.csv\")\n",
    "\n",
    "print(f\"\\nğŸ“ Creating proper metadata at: {proper_meta_path}\")\n",
    "\n",
    "# Get actual .wav files\n",
    "wav_files = [f for f in os.listdir(dataset_config.path) if f.lower().endswith('.wav')]\n",
    "print(f\"ğŸ“ Found {len(wav_files)} .wav files\")\n",
    "\n",
    "# Create proper metadata\n",
    "proper_lines = []\n",
    "for i, wav_file in enumerate(wav_files[:100]):  # Use first 100\n",
    "    # Create proper LJSpeech format: filename|text|normalized_text\n",
    "    # For testing, use simple text\n",
    "    text = f\"Test text number {i+1} for Nepali TTS\"\n",
    "    proper_lines.append(f\"{wav_file}|{text}|{text}\")  # Note: THREE parts with pipe\n",
    "\n",
    "# Write the file\n",
    "with open(proper_meta_path, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    f.write('\\n'.join(proper_lines))\n",
    "\n",
    "print(f\"âœ… Created proper metadata with {len(proper_lines)} entries\")\n",
    "\n",
    "# Verify the file\n",
    "print(\"\\nğŸ” Verifying proper metadata:\")\n",
    "with open(proper_meta_path, 'r', encoding='utf-8') as f:\n",
    "    verify_lines = f.readlines()\n",
    "\n",
    "print(f\"ğŸ“„ First 3 entries:\")\n",
    "for i in range(min(3, len(verify_lines))):\n",
    "    line = verify_lines[i].strip()\n",
    "    print(f\"  {i+1}. {line}\")\n",
    "\n",
    "# Update dataset config\n",
    "print(f\"\\nğŸ”„ Updating dataset config to use proper metadata\")\n",
    "dataset_config.meta_file_train = proper_meta_path\n",
    "\n",
    "# ===== TRY CUSTOM DATASET LOADING =====\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ”„ TRYING CUSTOM DATASET LOADING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Instead of using load_tts_samples, let's create samples manually\n",
    "print(\"\\nğŸ§ª Creating samples manually...\")\n",
    "\n",
    "def create_manual_samples(metadata_path, audio_dir):\n",
    "    \"\"\"Manually create samples to bypass formatter issues\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        parts = line.split('|')\n",
    "        if len(parts) >= 2:\n",
    "            filename = parts[0].strip()\n",
    "            text = parts[1].strip()\n",
    "            \n",
    "            # Skip if filename or text is empty\n",
    "            if not filename or not text:\n",
    "                continue\n",
    "                \n",
    "            # Fix double .wav extension if present\n",
    "            if filename.endswith('.wav.wav'):\n",
    "                filename = filename[:-4]  # Remove one .wav\n",
    "            \n",
    "            audio_path = os.path.join(audio_dir, filename)\n",
    "            \n",
    "            if os.path.exists(audio_path):\n",
    "                sample = {\n",
    "                    'audio_file': audio_path,\n",
    "                    'text': text,\n",
    "                    'speaker_name': 'nepali_speaker',\n",
    "                    'language': 'ne'\n",
    "                }\n",
    "                samples.append(sample)\n",
    "            else:\n",
    "                print(f\"âš ï¸ File not found: {audio_path}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Create manual samples\n",
    "train_samples = create_manual_samples(proper_meta_path, dataset_config.path)\n",
    "eval_samples = train_samples[:20]  # Use first 20 as validation\n",
    "\n",
    "print(f\"âœ… Manual samples created:\")\n",
    "print(f\"   Training samples: {len(train_samples)}\")\n",
    "print(f\"   Validation samples: {len(eval_samples)}\")\n",
    "\n",
    "if train_samples:\n",
    "    sample = train_samples[0]\n",
    "    print(f\"\\nğŸ“„ Sample from manual creation:\")\n",
    "    print(f\"   Audio file: {os.path.basename(sample['audio_file'])}\")\n",
    "    print(f\"   Text: '{sample['text']}'\")\n",
    "    print(f\"   Text length: {len(sample['text'])}\")\n",
    "    print(f\"   File exists: {os.path.exists(sample['audio_file'])}\")\n",
    "    \n",
    "    # Test tokenizer on the sample text\n",
    "    print(f\"\\nğŸ” Testing tokenizer on sample text:\")\n",
    "    test_ids = tokenizer.text_to_ids(sample['text'])\n",
    "    print(f\"   Token IDs: {len(test_ids)} tokens\")\n",
    "    print(f\"   Decoded: '{tokenizer.ids_to_text(test_ids)}'\")\n",
    "\n",
    "# ===== OVERRIDE THE DATASET LOADING =====\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸš€ OVERRIDING DATASET CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Since LJSpeech formatter seems problematic, let's try a different approach\n",
    "# We'll modify the dataset config to use a custom formatter\n",
    "\n",
    "# Update the dataset config to use our manual samples\n",
    "print(\"\\nğŸ“Š Using manually created samples for training\")\n",
    "\n",
    "# Skip the load_tts_samples step since we already have samples\n",
    "print(\"âœ… Bypassing load_tts_samples function\")\n",
    "\n",
    "# Continue with the rest of the training setup...\n",
    "print(\"\\nâ¡ï¸ Continuing with model creation and training...\")\n",
    "\n",
    "# ===== CONTINUE WITH THE REST OF THE CODE =====\n",
    "# (Keep all the code after STEP 8 the same, starting with Step 9: Audio Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0987d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸµ Creating audio configuration...\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      "âœ… Audio processor: 22050 Hz\n",
      "\n",
      "âš™ï¸ VITS configuration...\n",
      "âœ… Config created:\n",
      "   Run name: nepali_vits_20251215_154211\n",
      "   Batch size: 4\n",
      "   Learning rate: 0.0002\n",
      "   Characters: 164\n",
      "   Epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 9: AUDIO CONFIGURATION =====\n",
    "print(\"\\nğŸµ Creating audio configuration...\")\n",
    "\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=22050,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    fft_size=1024,\n",
    "    num_mels=80,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000.0,\n",
    ")\n",
    "\n",
    "# Create audio processor from config\n",
    "ap = AudioProcessor.init_from_config(audio_config)\n",
    "print(f\"âœ… Audio processor: {ap.sample_rate} Hz\")\n",
    "\n",
    "# ===== STEP 10: VITS CONFIGURATION =====\n",
    "print(\"\\nâš™ï¸ VITS configuration...\")\n",
    "\n",
    "config = VitsConfig(\n",
    "    output_path=OUTPUT,\n",
    "    run_name=f\"nepali_vits_{time.strftime('%Y%m%d_%H%M%S')}\",\n",
    ")\n",
    "\n",
    "# Set attributes\n",
    "config.datasets = [dataset_config]\n",
    "config.audio = audio_config\n",
    "\n",
    "# Training parameters\n",
    "config.batch_size = 4\n",
    "config.eval_batch_size = 2\n",
    "config.num_loader_workers = 0  # Set to 0 to avoid multiprocessing issues on Windows\n",
    "config.num_eval_loader_workers = 0\n",
    "config.epochs = 50  # Reduced for testing\n",
    "\n",
    "# Text processing\n",
    "config.text_cleaner = \"basic_cleaners\"\n",
    "config.use_phonemes = False\n",
    "config.add_blank = True\n",
    "config.characters = None\n",
    "config.num_chars = len(tokenizer.characters.characters)\n",
    "\n",
    "# Optimizer\n",
    "config.optimizer = \"AdamW\"\n",
    "config.optimizer_params = {\"betas\": [0.8, 0.99], \"eps\": 1e-9, \"weight_decay\": 0.01}\n",
    "config.lr = 2e-4\n",
    "config.lr_scheduler = \"ExponentialLR\"\n",
    "config.lr_scheduler_params = {\"gamma\": 0.999875}\n",
    "\n",
    "# Training monitoring\n",
    "config.print_step = 25\n",
    "config.save_step = 500  # Reduced for testing\n",
    "config.save_n_checkpoints = 3\n",
    "config.run_eval = True\n",
    "\n",
    "# Test sentences\n",
    "config.test_sentences = [\n",
    "    \"à¤¨à¤®à¤¸à¥à¤¤à¥‡\",\n",
    "    \"à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦\",\n",
    "    \"à¤•à¥‡ à¤›\",\n",
    "    \"à¤® à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤¹à¥à¤\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… Config created:\")\n",
    "print(f\"   Run name: {config.run_name}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Learning rate: {config.lr}\")\n",
    "print(f\"   Characters: {config.num_chars}\")\n",
    "print(f\"   Epochs: {config.epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e7db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Creating VITS model...\n",
      "ğŸ’» Using CPU (training will be slower)\n",
      "âœ… Model ready (83,068,204 parameters)\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 11: CREATE MODEL =====\n",
    "print(\"\\nğŸ§  Creating VITS model...\")\n",
    "\n",
    "model = Vits(\n",
    "    config=config,\n",
    "    ap=ap,\n",
    "    tokenizer=tokenizer,\n",
    "    speaker_manager=None,\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ğŸ® Moving model to GPU...\")\n",
    "    model.cuda()\n",
    "    print(f\"âœ… Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"ğŸ’» Using CPU (training will be slower)\")\n",
    "\n",
    "print(f\"âœ… Model ready ({sum(p.numel() for p in model.parameters()):,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Num. of CPUs: 16\n",
      " | > Num. of Torch Threads: 12\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¨â€ğŸ« Creating trainer with manual samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Start Tensorboard: tensorboard --logdir=C:\\Users\\ReticleX\\Pictures\\nepali_tts\\vits_output\\nepali_vits_20251215_154211-December-15-2025_03+43PM-0000000\n",
      "\n",
      " > Model has 83068204 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainer ready!\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 12: CREATE TRAINER WITH MANUAL SAMPLES =====\n",
    "print(\"\\nğŸ‘¨â€ğŸ« Creating trainer with manual samples...\")\n",
    "\n",
    "trainer_args = TrainerArgs(\n",
    "    continue_path=None,\n",
    "    restore_path=None,\n",
    "    use_ddp=False,  # Disable distributed training for simplicity\n",
    ")\n",
    "\n",
    "# Create trainer with our manually created samples\n",
    "trainer = Trainer(\n",
    "    trainer_args,\n",
    "    config,\n",
    "    OUTPUT,\n",
    "    model=model,\n",
    "    train_samples=train_samples,  # Use our manually created samples\n",
    "    eval_samples=eval_samples,    # Use our manually created samples\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a48e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Creating custom data loader...\n",
      "ğŸ“Š Creating datasets...\n",
      "   Original samples: 100\n",
      "   Filtered samples: 55\n",
      "   Original samples: 20\n",
      "   Filtered samples: 8\n",
      "âœ… Data loaders created:\n",
      "   Train batches: 14\n",
      "   Eval batches: 4\n",
      "\n",
      "ğŸ§ª Testing one batch...\n",
      "âœ… Batch loaded successfully!\n",
      "   Batch size: 4\n",
      "   Text shape: torch.Size([4, 69])\n",
      "   Text lengths: tensor([69, 69, 69, 67])\n",
      "   Audio shape: torch.Size([4, 1, 94976])\n",
      "   Audio lengths: tensor([87760, 55255, 56890, 94976])\n",
      "\n",
      "ğŸ§ª Testing model forward pass...\n",
      "   Standard signature failed: Vits.forward() got an unexpected keyword argument 'sid'\n",
      "âŒ Forward pass failed: Vits.forward() missing 1 required positional argument: 'waveform'\n",
      "\n",
      "ğŸ” Checking model forward method signature...\n",
      "   Forward signature: (x: <built-in method tensor of type object at 0x00007FFF9029E8C0>, x_lengths: <built-in method tensor of type object at 0x00007FFF9029E8C0>, y: <built-in method tensor of type object at 0x00007FFF9029E8C0>, y_lengths: <built-in method tensor of type object at 0x00007FFF9029E8C0>, waveform: <built-in method tensor of type object at 0x00007FFF9029E8C0>, aux_input={'d_vectors': None, 'speaker_ids': None, 'language_ids': None}) -> Dict\n",
      "   Docstring: Forward pass of the model.\n",
      "\n",
      "        Args:\n",
      "            x (torch.tensor): Batch of input character sequence IDs.\n",
      "            x_lengths (torch.tensor): Batch of input character sequence lengths.\n",
      "        ...\n",
      "\n",
      "ğŸ”„ Trying minimal forward pass...\n",
      "âŒ Minimal forward also failed: Vits.forward() missing 1 required positional argument: 'waveform'\n",
      "\n",
      "ğŸ”§ Creating custom training loop...\n",
      "ğŸ” Inspecting VITS model structure...\n",
      "\n",
      "ğŸ§ª Testing different forward signatures...\n",
      "âŒ Signature 1 failed: Given groups=1, weight of size [192, 513, 1], expected input[4, 1, 78998] to have 513 channels, but got 1 channels instead\n",
      "âŒ Signature 2 failed: Vits.forward() missing 1 required positional argument: 'waveform'\n",
      "â„¹ï¸ model.forward_train not available\n",
      "\n",
      "ğŸ§ª Final forward pass test...\n",
      "âŒ Forward test failed: Vits.forward() missing 1 required positional argument: 'waveform'\n",
      "\n",
      "ğŸ’¡ Manual workaround: Let's check the actual VITS source code...\n",
      "ğŸ” Model class source location:\n",
      "   TTS.tts.models.vits.Vits\n",
      "\n",
      "ğŸ“‹ Available methods in model:\n",
      "   - MODEL_TYPE\n",
      "   - T_destination\n",
      "   - add_module\n",
      "   - ap\n",
      "   - apply\n",
      "   - args\n",
      "   - audio_transform\n",
      "   - bfloat16\n",
      "   - buffers\n",
      "   - call_super_init\n",
      "\n",
      "âœ… Found train_step method! Using that...\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 13: CREATE CUSTOM DATA LOADER (FIXED) =====\n",
    "print(\"\\nğŸ” Creating custom data loader...\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class NepaliTTSDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Nepali TTS\"\"\"\n",
    "    def __init__(self, samples, tokenizer, ap, max_text_length=200, max_audio_length=100000):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ap = ap\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_audio_length = max_audio_length\n",
    "        \n",
    "        # Filter samples that are too long\n",
    "        self.filtered_samples = []\n",
    "        for sample in samples:\n",
    "            # Tokenize to check text length\n",
    "            token_ids = tokenizer.text_to_ids(sample['text'])\n",
    "            if len(token_ids) <= max_text_length:\n",
    "                # Check audio length\n",
    "                try:\n",
    "                    audio_info = os.stat(sample['audio_file'])\n",
    "                    # Approximate audio length from file size (16-bit mono at 22050 Hz)\n",
    "                    approx_length = audio_info.st_size / 2  # 2 bytes per sample for 16-bit\n",
    "                    if approx_length <= max_audio_length:\n",
    "                        self.filtered_samples.append(sample)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"   Original samples: {len(samples)}\")\n",
    "        print(f\"   Filtered samples: {len(self.filtered_samples)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filtered_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.filtered_samples[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        text = sample['text']\n",
    "        token_ids = self.tokenizer.text_to_ids(text)\n",
    "        \n",
    "        # Load and process audio\n",
    "        waveform = self.ap.load_wav(sample['audio_file'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        token_ids = torch.LongTensor(token_ids)\n",
    "        waveform = torch.FloatTensor(waveform)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'token_ids': token_ids,\n",
    "            'text_lengths': len(token_ids),\n",
    "            'waveform': waveform,\n",
    "            'waveform_lengths': len(waveform),\n",
    "            'speaker_id': 0,  # Single speaker\n",
    "            'audio_file': sample['audio_file'],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences\"\"\"\n",
    "    # Get max lengths in this batch\n",
    "    max_text_len = max(item['text_lengths'] for item in batch)\n",
    "    max_audio_len = max(item['waveform_lengths'] for item in batch)\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    text_inputs = torch.zeros(batch_size, max_text_len, dtype=torch.long)\n",
    "    text_lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    waveforms = torch.zeros(batch_size, max_audio_len, dtype=torch.float32)\n",
    "    waveform_lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    speaker_ids = torch.zeros(batch_size, dtype=torch.long)\n",
    "    audio_names = []\n",
    "    raw_texts = []\n",
    "    \n",
    "    # Fill tensors\n",
    "    for i, item in enumerate(batch):\n",
    "        text_len = item['text_lengths']\n",
    "        audio_len = item['waveform_lengths']\n",
    "        \n",
    "        # Text\n",
    "        text_inputs[i, :text_len] = item['token_ids']\n",
    "        text_lengths[i] = text_len\n",
    "        raw_texts.append(item['text'])\n",
    "        \n",
    "        # Audio\n",
    "        waveforms[i, :audio_len] = item['waveform']\n",
    "        waveform_lengths[i] = audio_len\n",
    "        \n",
    "        # Speaker (dummy)\n",
    "        speaker_ids[i] = item['speaker_id']\n",
    "        \n",
    "        # Audio name\n",
    "        audio_names.append(item['audio_file'])\n",
    "    \n",
    "    # Add channel dimension for audio (VITS expects [B, 1, T])\n",
    "    waveforms = waveforms.unsqueeze(1)\n",
    "    \n",
    "    return {\n",
    "        'text_input': text_inputs,\n",
    "        'text_lengths': text_lengths,\n",
    "        'waveform': waveforms,\n",
    "        'waveform_lengths': waveform_lengths,\n",
    "        'speaker_ids': speaker_ids,\n",
    "        'raw_text': raw_texts,\n",
    "        'audio_unique_name': audio_names,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "print(\"ğŸ“Š Creating datasets...\")\n",
    "train_dataset = NepaliTTSDataset(train_samples, tokenizer, ap)\n",
    "eval_dataset = NepaliTTSDataset(eval_samples, tokenizer, ap)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=config.eval_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data loaders created:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Eval batches: {len(eval_loader)}\")\n",
    "\n",
    "# Test one batch\n",
    "print(\"\\nğŸ§ª Testing one batch...\")\n",
    "for batch in train_loader:\n",
    "    print(f\"âœ… Batch loaded successfully!\")\n",
    "    print(f\"   Batch size: {batch['text_input'].shape[0]}\")\n",
    "    print(f\"   Text shape: {batch['text_input'].shape}\")\n",
    "    print(f\"   Text lengths: {batch['text_lengths']}\")\n",
    "    print(f\"   Audio shape: {batch['waveform'].shape}\")\n",
    "    print(f\"   Audio lengths: {batch['waveform_lengths']}\")\n",
    "    \n",
    "    # Test model forward pass\n",
    "    print(\"\\nğŸ§ª Testing model forward pass...\")\n",
    "    try:\n",
    "        # Move batch to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        # Get the correct forward method signature by checking the source\n",
    "        # Let's try different signatures\n",
    "        try:\n",
    "            # Try signature 1: Standard VITS forward\n",
    "            outputs = model.forward(\n",
    "                x=batch['text_input'],\n",
    "                x_lengths=batch['text_lengths'],\n",
    "                y=batch['waveform'],\n",
    "                y_lengths=batch['waveform_lengths'],\n",
    "                sid=batch['speaker_ids']\n",
    "            )\n",
    "            print(f\"âœ… Forward pass successful with standard signature!\")\n",
    "        except Exception as e1:\n",
    "            print(f\"   Standard signature failed: {e1}\")\n",
    "            \n",
    "            # Try signature 2: Without speaker IDs\n",
    "            outputs = model.forward(\n",
    "                x=batch['text_input'],\n",
    "                x_lengths=batch['text_lengths'],\n",
    "                y=batch['waveform'],\n",
    "                y_lengths=batch['waveform_lengths'],\n",
    "            )\n",
    "            print(f\"âœ… Forward pass successful without speaker IDs!\")\n",
    "        \n",
    "        print(f\"   Output keys: {list(outputs.keys())}\")\n",
    "        \n",
    "        # Check for losses\n",
    "        if hasattr(outputs, 'keys'):\n",
    "            for key in outputs.keys():\n",
    "                if 'loss' in key:\n",
    "                    print(f\"   {key}: {outputs[key].item():.4f}\")\n",
    "        elif isinstance(outputs, dict):\n",
    "            for key, value in outputs.items():\n",
    "                if 'loss' in key:\n",
    "                    print(f\"   {key}: {value.item():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Forward pass failed: {e}\")\n",
    "        \n",
    "        # Debug: Check model forward method signature\n",
    "        print(\"\\nğŸ” Checking model forward method signature...\")\n",
    "        import inspect\n",
    "        try:\n",
    "            sig = inspect.signature(model.forward)\n",
    "            print(f\"   Forward signature: {sig}\")\n",
    "            \n",
    "            # Show docstring\n",
    "            if model.forward.__doc__:\n",
    "                print(f\"   Docstring: {model.forward.__doc__[:200]}...\")\n",
    "        except:\n",
    "            print(\"   Could not inspect signature\")\n",
    "        \n",
    "        # Try one more time with minimal arguments\n",
    "        print(\"\\nğŸ”„ Trying minimal forward pass...\")\n",
    "        try:\n",
    "            outputs = model.forward(\n",
    "                batch['text_input'],\n",
    "                batch['text_lengths'],\n",
    "                batch['waveform'],\n",
    "                batch['waveform_lengths'],\n",
    "            )\n",
    "            print(f\"âœ… Minimal forward pass successful!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Minimal forward also failed: {e2}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "# ===== STEP 14: CREATE CUSTOM TRAINER WRAPPER (FIXED) =====\n",
    "print(\"\\nğŸ”§ Creating custom training loop...\")\n",
    "\n",
    "# Let's check the actual forward method by looking at the source\n",
    "print(\"ğŸ” Inspecting VITS model structure...\")\n",
    "\n",
    "# Try to get a batch and see what happens\n",
    "test_batch = next(iter(train_loader))\n",
    "if torch.cuda.is_available():\n",
    "    test_batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "\n",
    "# Try different forward signatures\n",
    "print(\"\\nğŸ§ª Testing different forward signatures...\")\n",
    "\n",
    "# Signature 1: Direct call\n",
    "try:\n",
    "    output1 = model(\n",
    "        test_batch['text_input'],\n",
    "        test_batch['text_lengths'],\n",
    "        test_batch['waveform'],\n",
    "        test_batch['waveform_lengths'],\n",
    "        test_batch['speaker_ids']\n",
    "    )\n",
    "    print(\"âœ… Signature 1 (model() with 5 args) works!\")\n",
    "    forward_signature = 1\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Signature 1 failed: {e}\")\n",
    "\n",
    "# Signature 2: Without speaker IDs\n",
    "try:\n",
    "    output2 = model(\n",
    "        test_batch['text_input'],\n",
    "        test_batch['text_lengths'],\n",
    "        test_batch['waveform'],\n",
    "        test_batch['waveform_lengths'],\n",
    "    )\n",
    "    print(\"âœ… Signature 2 (model() with 4 args) works!\")\n",
    "    forward_signature = 2\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Signature 2 failed: {e}\")\n",
    "\n",
    "# Signature 3: Using model.forward_train\n",
    "try:\n",
    "    if hasattr(model, 'forward_train'):\n",
    "        output3 = model.forward_train(\n",
    "            test_batch['text_input'],\n",
    "            test_batch['text_lengths'],\n",
    "            test_batch['waveform'],\n",
    "            test_batch['waveform_lengths'],\n",
    "            test_batch['speaker_ids']\n",
    "        )\n",
    "        print(\"âœ… Signature 3 (forward_train) works!\")\n",
    "        forward_signature = 3\n",
    "    else:\n",
    "        print(\"â„¹ï¸ model.forward_train not available\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Signature 3 failed: {e}\")\n",
    "\n",
    "# Based on which signature works, create the trainer\n",
    "class SimpleTrainer:\n",
    "    def __init__(self, model, train_loader, eval_loader, config, output_dir):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.eval_loader = eval_loader\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=config.optimizer_params.get('betas', [0.8, 0.99]),\n",
    "            eps=config.optimizer_params.get('eps', 1e-9),\n",
    "            weight_decay=config.optimizer_params.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            self.optimizer,\n",
    "            gamma=config.lr_scheduler_params.get('gamma', 0.999875)\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Determine which forward signature to use\n",
    "        self.forward_signature = 2  # Default to 4-argument version\n",
    "        \n",
    "        print(f\"âœ… SimpleTrainer initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Forward signature: {self.forward_signature}\")\n",
    "        print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "    def forward_model(self, batch):\n",
    "        \"\"\"Forward pass with the correct signature\"\"\"\n",
    "        if self.forward_signature == 1:\n",
    "            return self.model(\n",
    "                batch['text_input'],\n",
    "                batch['text_lengths'],\n",
    "                batch['waveform'],\n",
    "                batch['waveform_lengths'],\n",
    "                batch['speaker_ids']\n",
    "            )\n",
    "        else:  # signature 2\n",
    "            return self.model(\n",
    "                batch['text_input'],\n",
    "                batch['text_lengths'],\n",
    "                batch['waveform'],\n",
    "                batch['waveform_lengths']\n",
    "            )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.forward_model(batch)\n",
    "            \n",
    "            # Get loss - VITS usually returns a dict or tuple\n",
    "            if isinstance(outputs, dict):\n",
    "                if 'loss' in outputs:\n",
    "                    loss = outputs['loss']\n",
    "                else:\n",
    "                    # Try to find any loss in the dict\n",
    "                    for key in outputs:\n",
    "                        if 'loss' in key:\n",
    "                            loss = outputs[key]\n",
    "                            break\n",
    "                    else:\n",
    "                        loss = outputs[list(outputs.keys())[0]]  # First item\n",
    "            elif isinstance(outputs, tuple):\n",
    "                # Usually first element is loss\n",
    "                loss = outputs[0]\n",
    "            else:\n",
    "                # Assume outputs is the loss tensor\n",
    "                loss = outputs\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % self.config.print_step == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"   Step {self.global_step}: Loss = {avg_loss:.4f}\")\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if self.global_step % self.config.save_step == 0:\n",
    "                    self.save_checkpoint(f\"checkpoint_{self.global_step}\")\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_loader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.forward_model(batch)\n",
    "                \n",
    "                # Get loss\n",
    "                if isinstance(outputs, dict):\n",
    "                    if 'loss' in outputs:\n",
    "                        loss = outputs['loss']\n",
    "                    else:\n",
    "                        for key in outputs:\n",
    "                            if 'loss' in key:\n",
    "                                loss = outputs[key]\n",
    "                                break\n",
    "                        else:\n",
    "                            loss = outputs[list(outputs.keys())[0]]\n",
    "                elif isinstance(outputs, tuple):\n",
    "                    loss = outputs[0]\n",
    "                else:\n",
    "                    loss = outputs\n",
    "                    \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    def save_checkpoint(self, name):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint_path = os.path.join(self.output_dir, f\"{name}.pth\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epoch': self.current_epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'best_loss': self.best_loss,\n",
    "            'config': self.config.__dict__,\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"   ğŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    def fit(self, epochs):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"\\nğŸš€ Starting training for {epochs} epochs...\")\n",
    "        print(f\"   Training samples: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"   Validation samples: {len(self.eval_loader.dataset)}\")\n",
    "        print(f\"   Batch size: {self.config.batch_size}\")\n",
    "        print(f\"   Learning rate: {self.config.lr}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.current_epoch = epoch\n",
    "            print(f\"\\nğŸ“ˆ Epoch {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss = self.evaluate()\n",
    "            print(f\"   Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.save_checkpoint(\"best_model\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(f\"epoch_{epoch + 1}\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Training completed!\")\n",
    "        self.save_checkpoint(\"final_model\")\n",
    "\n",
    "# Test forward pass one more time to confirm\n",
    "print(\"\\nğŸ§ª Final forward pass test...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    if torch.cuda.is_available():\n",
    "        test_batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "    \n",
    "    # Try the 4-argument version (most likely)\n",
    "    outputs = model(\n",
    "        test_batch['text_input'],\n",
    "        test_batch['text_lengths'],\n",
    "        test_batch['waveform'],\n",
    "        test_batch['waveform_lengths']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Forward test successful!\")\n",
    "    \n",
    "    if isinstance(outputs, dict):\n",
    "        print(f\"   Output type: dict with keys: {list(outputs.keys())}\")\n",
    "        for key, value in outputs.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"   {key}: shape {value.shape}\")\n",
    "            elif torch.is_tensor(value):\n",
    "                print(f\"   {key}: tensor with value {value.item():.4f}\")\n",
    "    elif isinstance(outputs, tuple):\n",
    "        print(f\"   Output type: tuple with {len(outputs)} elements\")\n",
    "        for i, value in enumerate(outputs):\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"   [{i}]: shape {value.shape}\")\n",
    "            elif torch.is_tensor(value):\n",
    "                print(f\"   [{i}]: tensor with value {value.item():.4f}\")\n",
    "    \n",
    "    # Create simple trainer\n",
    "    print(\"\\nğŸ‘¨â€ğŸ« Creating SimpleTrainer...\")\n",
    "    simple_trainer = SimpleTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=eval_loader,\n",
    "        config=config,\n",
    "        output_dir=os.path.join(OUTPUT, config.run_name)\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Forward test failed: {e}\")\n",
    "    print(\"\\nğŸ’¡ Manual workaround: Let's check the actual VITS source code...\")\n",
    "    \n",
    "    # Try to import and check\n",
    "    import inspect\n",
    "    print(\"ğŸ” Model class source location:\")\n",
    "    print(f\"   {model.__class__.__module__}.{model.__class__.__name__}\")\n",
    "    \n",
    "    # Show available methods\n",
    "    print(\"\\nğŸ“‹ Available methods in model:\")\n",
    "    methods = [m for m in dir(model) if not m.startswith('_')]\n",
    "    for method in methods[:10]:  # Show first 10\n",
    "        print(f\"   - {method}\")\n",
    "    \n",
    "    # Check if there's a train_step method\n",
    "    if hasattr(model, 'train_step'):\n",
    "        print(\"\\nâœ… Found train_step method! Using that...\")\n",
    "    else:\n",
    "        print(\"\\nâ“ No train_step found. Let's try to call the model directly...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af6d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Creating custom training loop using train_step method...\n",
      "ğŸ§ª Testing train_step method...\n",
      "âŒ train_step failed: name 'train_loader' is not defined\n",
      "\n",
      "ğŸ‘¨â€ğŸ« Creating VitsTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ReticleX\\AppData\\Local\\Temp\\ipykernel_836\\3278468428.py\", line 8, in <module>\n",
      "    test_batch = next(iter(train_loader))\n",
      "NameError: name 'train_loader' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 360\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ‘¨â€ğŸ« Creating VitsTrainer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    359\u001b[0m vits_trainer \u001b[38;5;241m=\u001b[39m VitsTrainer(\n\u001b[1;32m--> 360\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[0;32m    361\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m    362\u001b[0m     eval_loader\u001b[38;5;241m=\u001b[39meval_loader,\n\u001b[0;32m    363\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    364\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT, config\u001b[38;5;241m.\u001b[39mrun_name),\n\u001b[0;32m    365\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m    366\u001b[0m     ap\u001b[38;5;241m=\u001b[39map\n\u001b[0;32m    367\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== STEP 14: CREATE CUSTOM TRAINER USING train_step METHOD =====\n",
    "print(\"\\nğŸ”§ Creating custom training loop using train_step method...\")\n",
    "\n",
    "# Test the train_step method\n",
    "print(\"ğŸ§ª Testing train_step method...\")\n",
    "\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    if torch.cuda.is_available():\n",
    "        test_batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "    \n",
    "    # Try train_step\n",
    "    outputs = model.train_step(\n",
    "        test_batch['text_input'],\n",
    "        test_batch['text_lengths'],\n",
    "        test_batch['waveform'],\n",
    "        test_batch['waveform_lengths']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… train_step successful!\")\n",
    "    \n",
    "    if isinstance(outputs, dict):\n",
    "        print(f\"   Output type: dict\")\n",
    "        print(f\"   Keys: {list(outputs.keys())}\")\n",
    "        \n",
    "        # Show loss values\n",
    "        for key, value in outputs.items():\n",
    "            if 'loss' in key.lower() and torch.is_tensor(value):\n",
    "                print(f\"   {key}: {value.item():.4f}\")\n",
    "    else:\n",
    "        print(f\"   Output type: {type(outputs)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ train_step failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Create trainer that uses train_step\n",
    "class VitsTrainer:\n",
    "    def __init__(self, model, train_loader, eval_loader, config, output_dir, tokenizer, ap):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.eval_loader = eval_loader\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ap = ap\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Setup optimizer based on config\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=config.optimizer_params.get('betas', [0.8, 0.99]),\n",
    "            eps=config.optimizer_params.get('eps', 1e-9),\n",
    "            weight_decay=config.optimizer_params.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            self.optimizer,\n",
    "            gamma=config.lr_scheduler_params.get('gamma', 0.999875)\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Loss tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        print(f\"âœ… VitsTrainer initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Training samples: {len(train_loader.dataset)}\")\n",
    "        print(f\"   Validation samples: {len(eval_loader.dataset)}\")\n",
    "        print(f\"   Batch size: {config.batch_size}\")\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass using train_step\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model.train_step(\n",
    "                batch['text_input'],\n",
    "                batch['text_lengths'],\n",
    "                batch['waveform'],\n",
    "                batch['waveform_lengths']\n",
    "            )\n",
    "            \n",
    "            # Extract loss - train_step returns a dict\n",
    "            if isinstance(outputs, dict):\n",
    "                # VITS train_step returns dict with 'loss' and other losses\n",
    "                if 'loss' in outputs:\n",
    "                    loss = outputs['loss']\n",
    "                else:\n",
    "                    # Try to find the main loss\n",
    "                    for key in outputs:\n",
    "                        if 'loss' in key and not key.startswith('loss_'):\n",
    "                            loss = outputs[key]\n",
    "                            break\n",
    "                    else:\n",
    "                        # Use the first loss found\n",
    "                        for key in outputs:\n",
    "                            if 'loss' in key:\n",
    "                                loss = outputs[key]\n",
    "                                break\n",
    "                        else:\n",
    "                            raise ValueError(\"No loss found in train_step outputs\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output type from train_step: {type(outputs)}\")\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % self.config.print_step == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                # Get detailed losses\n",
    "                loss_details = \"\"\n",
    "                if isinstance(outputs, dict):\n",
    "                    for key, value in outputs.items():\n",
    "                        if 'loss' in key and torch.is_tensor(value):\n",
    "                            loss_details += f\", {key}: {value.item():.4f}\"\n",
    "                \n",
    "                print(f\"   Step {self.global_step}: Loss = {avg_loss:.4f}, LR = {current_lr:.6f}{loss_details}\")\n",
    "                \n",
    "                # Log to tensorboard if available\n",
    "                if hasattr(self, 'writer'):\n",
    "                    self.writer.add_scalar('train/loss', avg_loss, self.global_step)\n",
    "                    for key, value in outputs.items():\n",
    "                        if 'loss' in key and torch.is_tensor(value):\n",
    "                            self.writer.add_scalar(f'train/{key}', value.item(), self.global_step)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if self.global_step % self.config.save_step == 0:\n",
    "                    self.save_checkpoint(f\"checkpoint_{self.global_step}\")\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_loader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model.train_step(\n",
    "                    batch['text_input'],\n",
    "                    batch['text_lengths'],\n",
    "                    batch['waveform'],\n",
    "                    batch['waveform_lengths']\n",
    "                )\n",
    "                \n",
    "                # Extract loss\n",
    "                if isinstance(outputs, dict):\n",
    "                    if 'loss' in outputs:\n",
    "                        loss = outputs['loss']\n",
    "                    else:\n",
    "                        for key in outputs:\n",
    "                            if 'loss' in key:\n",
    "                                loss = outputs[key]\n",
    "                                break\n",
    "                        else:\n",
    "                            loss = outputs[list(outputs.keys())[0]]\n",
    "                else:\n",
    "                    loss = outputs\n",
    "                    \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    def save_checkpoint(self, name):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint_path = os.path.join(self.output_dir, f\"{name}.pth\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epoch': self.current_epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'best_loss': self.best_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'config': self.config.__dict__,\n",
    "            'tokenizer_config': {\n",
    "                'vocab_size': len(self.tokenizer.characters.characters),\n",
    "                'characters': self.tokenizer.characters.characters,\n",
    "            },\n",
    "            'audio_config': {\n",
    "                'sample_rate': self.ap.sample_rate,\n",
    "                'num_mels': self.ap.num_mels,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"   ğŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Also save config separately\n",
    "        config_path = os.path.join(self.output_dir, \"config.json\")\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.config.__dict__, f, indent=2, default=str)\n",
    "    \n",
    "    def fit(self, epochs):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"\\nğŸš€ Starting training for {epochs} epochs...\")\n",
    "        print(f\"   Total steps per epoch: {len(self.train_loader)}\")\n",
    "        print(f\"   Total training steps: {len(self.train_loader) * epochs}\")\n",
    "        print(f\"   Learning rate: {self.config.lr}\")\n",
    "        print(f\"   Output directory: {self.output_dir}\")\n",
    "        \n",
    "        # Try to setup tensorboard\n",
    "        try:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            self.writer = SummaryWriter(log_dir=self.output_dir)\n",
    "            print(f\"   TensorBoard: tensorboard --logdir={self.output_dir}\")\n",
    "        except:\n",
    "            print(\"   TensorBoard not available, skipping...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.current_epoch = epoch\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ“ˆ Epoch {epoch + 1}/{epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss = self.evaluate()\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Epoch {epoch + 1} Summary:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"   Val Loss:   {val_loss:.4f}\")\n",
    "            print(f\"   Epoch Time: {epoch_time:.1f}s\")\n",
    "            print(f\"   Total Time: {total_time:.1f}s\")\n",
    "            print(f\"   Global Step: {self.global_step}\")\n",
    "            print(f\"   Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Log to tensorboard\n",
    "            if hasattr(self, 'writer'):\n",
    "                self.writer.add_scalar('epoch/train_loss', train_loss, epoch)\n",
    "                self.writer.add_scalar('epoch/val_loss', val_loss, epoch)\n",
    "                self.writer.add_scalar('epoch/lr', self.optimizer.param_groups[0]['lr'], epoch)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.save_checkpoint(\"best_model\")\n",
    "                print(f\"   ğŸ† New best model! Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "                self.save_checkpoint(f\"epoch_{epoch + 1}\")\n",
    "            \n",
    "            # Generate test samples every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.generate_test_samples(epoch + 1)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ‰ TRAINING COMPLETED!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   Total epochs: {epochs}\")\n",
    "        print(f\"   Total steps: {self.global_step}\")\n",
    "        print(f\"   Best validation loss: {self.best_loss:.4f}\")\n",
    "        print(f\"   Total training time: {time.time() - start_time:.1f}s\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_checkpoint(\"final_model\")\n",
    "        \n",
    "        # Close tensorboard writer\n",
    "        if hasattr(self, 'writer'):\n",
    "            self.writer.close()\n",
    "    \n",
    "    def generate_test_samples(self, epoch):\n",
    "        \"\"\"Generate test audio samples\"\"\"\n",
    "        print(f\"\\nğŸ¤ Generating test samples for epoch {epoch}...\")\n",
    "        \n",
    "        test_dir = os.path.join(self.output_dir, \"test_samples\", f\"epoch_{epoch}\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "        \n",
    "        test_texts = [\n",
    "            \"à¤¨à¤®à¤¸à¥à¤¤à¥‡\",\n",
    "            \"à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦\",\n",
    "            \"à¤•à¥‡ à¤›\",\n",
    "            \"à¤® à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤¹à¥à¤\"\n",
    "        ]\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for i, text in enumerate(test_texts):\n",
    "            try:\n",
    "                # Use model's inference method\n",
    "                outputs = self.model.inference(text)\n",
    "                audio = outputs[\"model_outputs\"].squeeze().cpu().numpy()\n",
    "                \n",
    "                # Save audio\n",
    "                import soundfile as sf\n",
    "                test_file = os.path.join(test_dir, f\"test_{i:02d}_{text[:10]}.wav\".replace(' ', '_'))\n",
    "                sf.write(test_file, audio, self.ap.sample_rate)\n",
    "                \n",
    "                print(f\"   âœ… '{text}' â†’ {test_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Failed to generate '{text}': {e}\")\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "# Create trainer\n",
    "print(\"\\nğŸ‘¨â€ğŸ« Creating VitsTrainer...\")\n",
    "vits_trainer = VitsTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    config=config,\n",
    "    output_dir=os.path.join(OUTPUT, config.run_name),\n",
    "    tokenizer=tokenizer,\n",
    "    ap=ap\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
